from tokenizers import Tokenizer

# hyperparameters
dataset = 'wiki'
# dataset = 'gutenberg'


text = "This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?! 😍"


saveFiles = {
    'wiki': 'tokenizer-wiki.json',
    'gutenberg': 'tokenizer-gutenberg.json'
}


tokenizer = Tokenizer.from_file(saveFiles[dataset])

print(text + '\n')
output = tokenizer.encode(text)
print(str(output.tokens) + '\n')

print(str(output.ids) + '\n')

print('Token count: ' + str(len(output.ids)))
