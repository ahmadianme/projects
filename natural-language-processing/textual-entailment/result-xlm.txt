Downloading: 100%
615/615 [00:00<00:00, 15.0kB/s]
Downloading: 100%
4.83M/4.83M [00:00<00:00, 4.92MB/s]
Downloading: 100%
8.68M/8.68M [00:00<00:00, 19.0MB/s]
Downloading: 100%
1.04G/1.04G [00:46<00:00, 30.4MB/s]

Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,

XLMRobertaForSequenceClassification(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(250002, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): RobertaEncoder(
      (layer): ModuleList(
        (0): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): RobertaLayer(
          (attention): RobertaAttention(
            (self): RobertaSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): RobertaSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): RobertaIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): RobertaOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): RobertaClassificationHead(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=768, out_features=3, bias=True)
  )
)

100%
30/30 [20:36<00:00, 41.30s/it]
Epoch 1: 100%
48/48 [00:33<00:00, 1.78it/s, training_loss=0.465]


Epoch 1
Training loss: 1.1025689380864303
Class: c: 0.009174311926605505
Class: n: 0.0
Class: e: 1.0
Validation loss: 1.141093506532557
Validation F1 Score (Weighted): 0.13752442230284234

Epoch 2: 100%
48/48 [00:35<00:00, 1.70it/s, training_loss=0.333]


Epoch 2
Training loss: 1.0909009526173274
Class: c: 0.0
Class: n: 0.0
Class: e: 1.0
Validation loss: 1.1468215058831608
Validation F1 Score (Weighted): 0.12950191570881225

Epoch 3: 100%
48/48 [00:36<00:00, 1.62it/s, training_loss=0.447]


Epoch 3
Training loss: 1.0866540831824143
Class: c: 0.0
Class: n: 0.024096385542168676
Class: e: 0.9743589743589743
Validation loss: 1.1351984739303589
Validation F1 Score (Weighted): 0.14258765009036847

Epoch 4: 100%
48/48 [00:37<00:00, 1.64it/s, training_loss=0.175]


Epoch 4
Training loss: 1.0432421614726384
Class: c: 0.13761467889908258
Class: n: 0.40963855421686746
Class: e: 0.6923076923076923
Validation loss: 1.1150879894985872
Validation F1 Score (Weighted): 0.34917952686708914

Epoch 5: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.378]


Epoch 5
Training loss: 0.9685108847916126
Class: c: 0.3119266055045872
Class: n: 0.3855421686746988
Class: e: 0.6410256410256411
Validation loss: 1.0596373887623058
Validation F1 Score (Weighted): 0.4271662749497306

Epoch 6: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.316]


Epoch 6
Training loss: 0.8937143372992674
Class: c: 0.43119266055045874
Class: n: 0.6024096385542169
Class: e: 0.48717948717948717
Validation loss: 1.0343253016471863
Validation F1 Score (Weighted): 0.5014705672975578

Epoch 7: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.175]


Epoch 7
Training loss: 0.7317892629653215
Class: c: 0.5871559633027523
Class: n: 0.43373493975903615
Class: e: 0.5
Validation loss: 1.086023393799277
Validation F1 Score (Weighted): 0.5151739933691172

Epoch 8: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.152]


Epoch 8
Training loss: 0.6591532242794832
Class: c: 0.3577981651376147
Class: n: 0.4578313253012048
Class: e: 0.7051282051282052
Validation loss: 1.2032660526387833
Validation F1 Score (Weighted): 0.4877475511146669

Epoch 9: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.236]


Epoch 9
Training loss: 0.5370045776168505
Class: c: 0.4036697247706422
Class: n: 0.7469879518072289
Class: e: 0.44871794871794873
Validation loss: 1.224914757644429
Validation F1 Score (Weighted): 0.5154228445840552

Epoch 10: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.156]


Epoch 10
Training loss: 0.4360227870444457
Class: c: 0.4954128440366973
Class: n: 0.5542168674698795
Class: e: 0.5128205128205128
Validation loss: 1.2346073669545792
Validation F1 Score (Weighted): 0.5218585159244501

Epoch 11: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.025]


Epoch 11
Training loss: 0.3081677778003116
Class: c: 0.4954128440366973
Class: n: 0.5783132530120482
Class: e: 0.5
Validation loss: 1.4182502171572517
Validation F1 Score (Weighted): 0.5243126423614886

Epoch 12: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.014]


Epoch 12
Training loss: 0.2326788459904492
Class: c: 0.5137614678899083
Class: n: 0.5783132530120482
Class: e: 0.41025641025641024
Validation loss: 1.5636332035064697
Validation F1 Score (Weighted): 0.5052481627355999

Epoch 13: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.011]


Epoch 13
Training loss: 0.1881459969251106
Class: c: 0.5596330275229358
Class: n: 0.5060240963855421
Class: e: 0.46153846153846156
Validation loss: 1.7037579452290255
Validation F1 Score (Weighted): 0.5173684304980428

Epoch 14: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.041]


Epoch 14
Training loss: 0.13550929628157368
Class: c: 0.5596330275229358
Class: n: 0.5662650602409639
Class: e: 0.41025641025641024
Validation loss: 1.9737901757745182
Validation F1 Score (Weighted): 0.518021865897162

Epoch 15: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.030]


Epoch 15
Training loss: 0.12596652389038354
Class: c: 0.5045871559633027
Class: n: 0.5783132530120482
Class: e: 0.4230769230769231
Validation loss: 2.0386173795251286
Validation F1 Score (Weighted): 0.504982929740649

Epoch 16: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.045]


Epoch 16
Training loss: 0.08687627311640729
Class: c: 0.5504587155963303
Class: n: 0.5542168674698795
Class: e: 0.41025641025641024
Validation loss: 2.19608534784878
Validation F1 Score (Weighted): 0.5115794504181601

Epoch 17: 100%
48/48 [00:37<00:00, 1.64it/s, training_loss=0.003]


Epoch 17
Training loss: 0.06868765606001641
Class: c: 0.5504587155963303
Class: n: 0.5301204819277109
Class: e: 0.5128205128205128
Validation loss: 2.3030218026217293
Validation F1 Score (Weighted): 0.5357096171802055

Epoch 18: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.003]


Epoch 18
Training loss: 0.03215291120189553
Class: c: 0.5596330275229358
Class: n: 0.4939759036144578
Class: e: 0.44871794871794873
Validation loss: 2.5541912387399113
Validation F1 Score (Weighted): 0.509978954808328

Epoch 19: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.001]


Epoch 19
Training loss: 0.03330285876290873
Class: c: 0.5871559633027523
Class: n: 0.5542168674698795
Class: e: 0.4358974358974359
Validation loss: 2.6186905888950123
Validation F1 Score (Weighted): 0.5335947677080984

Epoch 20: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.002]


Epoch 20
Training loss: 0.026959789218381047
Class: c: 0.5137614678899083
Class: n: 0.4939759036144578
Class: e: 0.6025641025641025
Validation loss: 2.673917868558098
Validation F1 Score (Weighted): 0.5359890644304011

Epoch 21: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.002]


Epoch 21
Training loss: 0.031071032300436247
Class: c: 0.5504587155963303
Class: n: 0.6024096385542169
Class: e: 0.3974358974358974
Validation loss: 2.907688288127675
Validation F1 Score (Weighted): 0.5223659612817954

Epoch 22: 100%
48/48 [00:37<00:00, 1.64it/s, training_loss=0.001]


Epoch 22
Training loss: 0.012831887380646853
Class: c: 0.5871559633027523
Class: n: 0.5180722891566265
Class: e: 0.41025641025641024
Validation loss: 2.8771794333177456
Validation F1 Score (Weighted): 0.5141015266505462

Epoch 23: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.000]


Epoch 23
Training loss: 0.016522408889917035
Class: c: 0.5871559633027523
Class: n: 0.5301204819277109
Class: e: 0.47435897435897434
Validation loss: 2.942034875645357
Validation F1 Score (Weighted): 0.5380417099342333

Epoch 24: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.001]


Epoch 24
Training loss: 0.007031118557885445
Class: c: 0.5963302752293578
Class: n: 0.5662650602409639
Class: e: 0.3974358974358974
Validation loss: 3.019951813361224
Validation F1 Score (Weighted): 0.5283003141986486

Epoch 25: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.001]


Epoch 25
Training loss: 0.015074827395437751
Class: c: 0.5963302752293578
Class: n: 0.5301204819277109
Class: e: 0.4358974358974359
Validation loss: 3.0112191438674927
Validation F1 Score (Weighted): 0.5289513633347401

Epoch 26: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.001]


Epoch 26
Training loss: 0.00799699117972826
Class: c: 0.5779816513761468
Class: n: 0.5421686746987951
Class: e: 0.4358974358974359
Validation loss: 3.0463769365759457
Validation F1 Score (Weighted): 0.5262264276240126

Epoch 27: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.005]


Epoch 27
Training loss: 0.014616352624822563
Class: c: 0.5504587155963303
Class: n: 0.5180722891566265
Class: e: 0.46153846153846156
Validation loss: 3.0753014718785003
Validation F1 Score (Weighted): 0.5160582349314744

Epoch 28: 100%
48/48 [00:37<00:00, 1.62it/s, training_loss=0.000]


Epoch 28
Training loss: 0.012982811400434002
Class: c: 0.5871559633027523
Class: n: 0.5421686746987951
Class: e: 0.4358974358974359
Validation loss: 3.114227897980634
Validation F1 Score (Weighted): 0.5297112333596551

Epoch 29: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.000]


Epoch 29
Training loss: 0.009968642761426358
Class: c: 0.5688073394495413
Class: n: 0.5421686746987951
Class: e: 0.44871794871794873
Validation loss: 3.1346666111665615
Validation F1 Score (Weighted): 0.5267174554529118

Epoch 30: 100%
48/48 [00:37<00:00, 1.63it/s, training_loss=0.000]


Epoch 30
Training loss: 0.013733377619549477
Class: c: 0.5688073394495413
Class: n: 0.5421686746987951
Class: e: 0.44871794871794873
Validation loss: 3.1239186455221737
Validation F1 Score (Weighted): 0.5267174554529118

Best validation loss in epoch: 6


Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Class: c: 0.3422459893048128
Class: n: 0.5936254980079682
Class: e: 0.5557377049180328
Test F1 Score (Weighted): 0.48893145634785107
